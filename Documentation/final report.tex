\documentclass[12pt]{article}

\usepackage[margin=1 in]{geometry}

%Use Times New Roman font
%\usepackage{pslatex}

% allow text colors
\usepackage[usenames,dvipsnames,svgnames,table,x11names]{xcolor}

%allow double-spacing
\usepackage{setspace}

%enable support for figures
\usepackage{graphicx}

\usepackage{rotating}
\usepackage{multirow}

\usepackage{url}

%handle filenames better
\usepackage{grffile}

%math
%\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{lipsum}

%control figure movement
\usepackage{placeins}

%center captions
\usepackage{caption}

%circuits and units
\usepackage[free-standing-units]{siunitx}
\DeclareSIUnit\vrms{\volt{}_{RMS}}
%\usepackage[americanvoltages,americancurrents]{circuitikz}
%\usetikzlibrary{calc}

% make scientific notation easy
\providecommand{\e}[1]{\ensuremath{\times 10^{#1}}}

% include pdf documents
\usepackage{pdfpages}


%for loops
\usepackage{pgffor}

%indent after new sections
\usepackage{indentfirst}

\usepackage{verbatim}

% allow greek letters outside math mode. \text<name of letter>
%\usepackage{textgreek}

% margin editing
\usepackage{changepage}

% braket notation, other goodies
\usepackage{physics}
\usepackage{braket}

%source code
\usepackage{listings}

%hyperlink support
\usepackage{hyperref}

% bibtex support
\usepackage{natbib}

%settings for Python code
\lstset{basicstyle=\footnotesize\ttfamily,
commentstyle=\color{OliveGreen},
keywordstyle=\color{blue},
tabsize=4,
numbers=none,
stringstyle=\color{red},
language=Python,
inputencoding=utf8,
extendedchars=true,
showstringspaces=flase,
}

\begin{document}
\singlespacing
\title{Final Report\\
AST 383}
\date{Dec 15 2015}
\author{Robert Rosati \\ Mar\'{i}a Jos\'{e} Bustamante Rosell}
\maketitle

\begin{abstract}
\par Case comparison of the robustness a top of the line density-based clustering algorithm, \texttt{DBSCAN},  to another one recently proposed, \texttt{FIDEPE}, under variations of user-defined parameters.
\end{abstract}

\doublespacing
\section{Motivation}

\par For our final project we decided to implement and compare the robustness of two density-based clustering algorithms under variations of user-defined parameters. 
\par Clustering algorithms have a wide variety of applications that range from satellite image segmentation, noise filtering and outlier detection, prediction of stock prices to bioinformatics. In particular, they are of high importance in astronomy when trying to identify dark matter halos and sub-halo structure.
\par There is a wide range of clustering algorithms out there for the taking. When handling large amounts of particles (in the order of trillions) the question of simplicity an easy parallelization arises also as a defining criterion. Both of the algorithms compared here are quite simple and easy to parallelize. \texttt{DBSCAN} is currently implemented on the latest halo-finders being developed \cite{BDCATS} and \texttt{FIDEPE}, being quite a new algorithm, still has no implementation on cosmology (at least that we know of), the more reason to explore the code's performance.

\section{Clustering Algorithms}



\subsection{\texttt{DBSCAN}}

The \texttt{DBSCAN} (Density-Based Spatial Clustering of Applications with Noise) algorithm is a version of \texttt{FOF} (Friends of Friends) in which, unlike the former, the local density of a point is taken into consideration. This algorithm classifies points as being core points, density-reachable points and outliers. This classification is based on two user defined parameters: the radius of your neighborhood (\texttt{eps}) and the minimum amount of points within it (\texttt{MinPts}). \cite{DBSCAN}

\begin{itemize}
\item A point $p$ is a core point if at least \texttt{MinPts} points are within distance \texttt{eps} of it. Those points are said to be directly reachable from $p$ (in it's neighborhood). No points are reachable from a non-core point.
\item A point $q$ is reachable from $p$ if there is a path $p_1, ..., p_n$ with $p_1 = p$ and $p_n = q$, where each $p_{i+1}$ is directly reachable from $p_i $ (so all the points on the path must be core points, with the possible exception of $q$).
\item All points not reachable from any other point are outliers which we mark as noise.
\end{itemize}

Once a core point is selected, all the points reachable from it will be tagged as part of the same cluster.

\FloatBarrier

\begin{figure}[h]
\centering
\includegraphics[width=0.8\linewidth]{DBSCAN-Illustration}
\caption{How \texttt{DBSCAN} selects the points in a cluster.}
\label{fig:DBSCAN}
\end{figure}\
\FloatBarrier
Figure \ref{fig:DBSCAN} \cite{wiki:dbscan} shows a version of DBSCAN with \texttt{MinPts} $= 3$. Point A and the other red points are core points, because at least three points surround each within an \texttt{eps} radius. Because they are all reachable from one another, they form a single cluster. Points B and C are not core points, but are reachable from A (via other core points) and thus belong to the cluster as well. Point N is a noise point that is neither a core point nor density-reachable.
\par For completeness, we have included the Python code for \texttt{DBSCAN} we used in this project below. Routines not crucial to the core of the algorithm have been omitted. For further information including full source code please visit our GitHub repository.\cite{git}

\begin{center}
\texttt{DBSCAN}
\end{center}
\singlespacing
\lstinputlisting{../Scripts/dbscan.py}
\doublespacing
\par The density checks mentioned above are implemented as a call to \texttt{regionQuery}, which populates the \texttt{point.neighbors} list. Clusters grow, after getting identified, by a call to \texttt{expandCluster}, which marks a point as noise if it has too few neighbors, or otherwise adds it to a list of core points that will be recursively expanded if the density requirements are met at each iteration.
% when talking about code
\par It's worth noting a few optimizations that can be made to our implementation of this algorithm.
The distances between points, instead of being computed for each point ($O(N^2)$ lookups), can be computed in a type of tree structure ($O(N \log(N))$ lookups). Such position-tree structures are often called R-trees or R*-trees.
In addition, the \texttt{expandCluster} routine is only called from a single location, so it may be safely inlined. In the interest of only surveying these clustering algorithms, we have not pursued these optimizations.

%prediction under varying parameters
\pagebreak

\subsection{FIDEPE}

\texttt{FIDEPE} (FInd DEnsity PEaks) algorithm \cite{FIDEPE} considers two criteria per point: local density and minimum distance to a point of higher density.

Local density ($\boldsymbol{\rho}$), is defined as the number of points $j$ in the ``neighborhood": points closer than $d_c$ from point $i$.
\begin{align*}
	\rho_i = \sum_j \chi(d_{ij}-d_c)
\end{align*}
where 
\begin{align*}
\chi(x)=
\begin{cases}
1 & x < 0 \\
0 & \textrm{otherwise}
\end{cases}
\end{align*}
Minimum distance to a point of higher density ($\boldsymbol{\delta}$) is self explanatory. 
\begin{align*}
	\delta_i = \text{min}_{j|\rho_j > \rho_i} (d_{ij})
\end{align*}
For the point of highest density, however, $\boldsymbol{\delta}$ is defined to be the maximum distance to another point.
\begin{align*}
	\delta_i = \text{max}_{j} (d_{ij})
\end{align*}
\FloatBarrier
\begin{figure}[ht]
\centering
\includegraphics[width=0.8\linewidth]{../plots/fidepe/FIDEPE}
\caption{How the \texttt{FIDEPE} algorithm identifies points in a cluster.}
\label{fig:FIDEPE}
\end{figure}
\FloatBarrier
Figure \ref{fig:FIDEPE} shows a simple example of the quantities described above. Here $\boldsymbol{\rho}$ is marked as numbers above each of the points, the circles around them define a neighborhood of radius $d_c$ and the directed arrows are $\boldsymbol{\delta}$.
\par Once these two quantities are defined for each point, the algorithm identifies the points with both the highest $\boldsymbol{\rho}$ and $\boldsymbol{\delta}$ and tags them as cluster centers. The choice for how many of them fall under this criterion is not specified in the original code. We decided to use the number of standard deviations ($\sigma$) away from the mean for $\boldsymbol{\delta}$ and $\boldsymbol{\rho}$ as our classifier. The number of standard deviations (\texttt{(nstddelta}) we chose turned out to greatly impact algorithm performance. Plots of \texttt{FIDEPE}'s performance under variation of this parameter and $d_c$ are available below.

Below you can find our implementation of \texttt{FIDEPE} as described above. As above, we are only attaching the functions that hold the core of the clustering mechanism. For further insight please see \cite{git}.

\singlespacing
\begin{center}
\texttt{FIDEPE}
\end{center}
\lstinputlisting{../Scripts/fidepe.py}
\doublespacing
\newgeometry{margin=0.5in}
\section{Plots}
%\begin{figure}[ht]
%\centering
%\raisebox{0.75in}{\rotatebox[]{90}{Aggregation}}
%\includegraphics[width=0.35\linewidth]{plots/Aggregation_eps} \includegraphics[width=0.35\linewidth]{plots/Aggregation_minpts} \\
%\raisebox{0.75in}{\rotatebox[]{90}{Compound}}
%\includegraphics[width=0.35\linewidth]{plots/Compound_eps} \includegraphics[width=0.35\linewidth]{plots/Compound_minpts} \\
%\raisebox{0.75in}{\rotatebox[]{90}{D31}}
%\includegraphics[width=0.35\linewidth]{plots/D31_eps}
%\includegraphics[width=0.35\linewidth]{plots/D31_minpts} \\
%\raisebox{0.75in}{\rotatebox[]{90}{jain}}
%\includegraphics[width=0.35\linewidth]{plots/jain_eps}
%\includegraphics[width=0.35\linewidth]{plots/jain_minpts} \\
%\raisebox{0.75in}{\rotatebox[]{90}{spiral}}
%\includegraphics[width=0.35\linewidth]{plots/spiral_eps}
%\includegraphics[width=0.35\linewidth]{plots/spiral_minpts}
%\caption{(Left) The effects of varying the \texttt{eps} parameter of \texttt{DBSCAN} for all reference distributions. \\ 
%(Right) The effects of varying the \texttt{MinPts} parameter of \texttt{DBSCAN}}.
%\label{fig:DBSCANplots}
%\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.47\linewidth]{../plots/datasets/Aggregation}
\includegraphics[width=0.47\linewidth]{../plots/datasets/Compound} \\
\includegraphics[width=0.47\linewidth]{../plots/datasets/jain}
\includegraphics[width=0.47\linewidth]{../plots/datasets/spiral}
\caption{The test datasets used in this analysis, from \cite{datasets}. The points are highlighted by (canonical) cluster membership. For reference, the canonical number of clusters for each dataset is, from left-to-right and top-to-bottom, 7,5,2, and 3.}
\label{fig:distributions}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.47\linewidth]{../plots/dbscan/Aggregation_DBSCAN}
\includegraphics[width=0.47\linewidth]{../plots/dbscan/Compound_DBSCAN} \\
\includegraphics[width=0.47\linewidth]{../plots/dbscan/jain_DBSCAN}
\includegraphics[width=0.47\linewidth]{../plots/dbscan/spiral_DBSCAN}
\caption{Number of clusters detected plotted versus the \texttt{DBSCAN} parameters. The plots above are in the same order as their corresponding datasets in Figure \ref{fig:distributions}. Note that the accepted number of clusters is reached in a fairly wide band in most plots.}
\label{fig:DBSCANplots}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.47\linewidth]{../plots/fidepe/Aggregation_FIDEPE}
\includegraphics[width=0.47\linewidth]{../plots/fidepe/Compound_FIDEPE} \\
\includegraphics[width=0.47\linewidth]{../plots/fidepe/jain_FIDEPE}
\includegraphics[width=0.47\linewidth]{../plots/fidepe/spiral_FIDEPE}
\caption{Number of clusters detected plotted versus the \texttt{FIDEPE} parameters. The plots above are in the same order as their corresponding datasets in Figure \ref{fig:distributions}.}
\label{fig:FIDPEPEplots}
\end{figure}
\restoregeometry
\doublespacing

\section{Results}

\par Initially, we expected to find some type of flat region in the number of clusters as we varied these algorithms' user-defined parameters. Were our algorithms robust to variations, we would have expected that the true clustering had some margin of error -- i.e., true clusters would tend to remain in well-defined high-density regions, while false clusters would tend to quickly merge or split. Sadly, this wasn't the case for either of our algorithms.
\par The \texttt{DBSCAN} algorithm seems to have less of a strong dependence on variations on the \texttt{MinPts} parameter in comparison to \texttt{eps}. For \texttt{DBSCAN}, among the datasets used, successful identification of the number of clusters was achieved over a wider range of parameters only for the \texttt{jain} dataset. Having said that though, the behavior for all the test cases remains fairly linear under variations of the parameters, which might be correlated to the fact that \texttt{DBSCAN} tends to append neighboring clusters together as the parameters vary.
\par On the other hand, the \texttt{FIDEPE} algorithm shows huge variations depending on our selection criteria for cluster centers (number of standard deviations from the mean of $\delta$). This might be an artifact of a poor selection criteria on our side. Nevertheless, plots also show significant variations trends regarding the cutoff distance, a key parameter in the original algorithm. Inspection by eye of the clustering identifications shows that there was a significant amount of miscounts, in which even though the right number of clusters was achieved, the algorithm failed to identify the actual clusters; unlike  \texttt{DBSCAN} it showed non-linear behavior under such variations.
\par Animated, colored plots of the datasets under several variations of these parameters are available in our GitHub repository. \cite{git}


%
%Referencing Figure \ref{fig:DBSCANplots}, the \texttt{DBSCAN} algorithm does not seem particularly robust to variation in either $\boldsymbol{\epsilon}$ or \texttt{MinPts}. 
%
%
%Referencing Figure \ref{fig:DBSCANplots}, the \texttt{DBSCAN} algorithm does not seem particularly robust to variation in either $\boldsymbol{\epsilon}$ or \texttt{MinPts}. Our proposed mechanism for objectively identifying the number of active clusters appears to fail in almost all test cases. The \texttt{spiral} dataset seems to be the exception, in that the flattest regions of the plots occur at the true number of clusters. The most densely clustered dataset, \texttt{D31}, converges in the range of \texttt{MinPts} plotted to the true cluster number, but fails to plateau in the range of \texttt{eps} plotted. (Describe other datasets?)
%\par Referencing Figure \ref{fig:FIDPEPEplots}, the \texttt{FIDEPE} algorithm is extremely sensitive to the user's choice of parameters. This may be to our ad hoc choice of the standard deviation clustering parameter. More.
%
%\par Our choice not to explore the full parameter space for these plots likely has contributed to our inability to find the peak cluster plateau we na\"\i vely expect. However, there is no guarantee of any sort of robustness in these algorithms. It may possible to construct datasets with a plateau on a false cluster number somewhere in the parameter space. We leave these questions for future work, now only concluding that our heuristic is useful in some nonzero percentage of typical datasets.
\clearpage
\bibliographystyle{plain}
\bibliography{research}

\end{document}
